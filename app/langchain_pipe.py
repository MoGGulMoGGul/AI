# app/langchain_pipe.py

import os
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import PGVector
from langchain_community.embeddings import OpenAIEmbeddings

from app.elasticsearch_client import index_tip_document  # âœ… Elasticsearch ì—°ë™ ì¶”ê°€
from app.ai_utils import summarize_and_tag  # âœ… ai_utilsë¡œ ì´ë™ëœ ìš”ì•½ ë° íƒœê·¸ í•¨ìˆ˜ ì‚¬ìš©

load_dotenv()

def run_langchain_pipeline(raw_text: str):
    # 1ï¸âƒ£ í…ìŠ¤íŠ¸ ìª¼ê°œê¸°
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.create_documents([raw_text])

    # 2ï¸âƒ£ ë²¡í„°í™”
    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

    # 3ï¸âƒ£ pgvector ì €ì¥
    vectorstore = PGVector.from_documents(
        documents=docs,
        embedding=embeddings,
        collection_name="tip_data",  # ğŸ‘‰ í•„ìš”ì‹œ ì»¬ë ‰ì…˜ëª…ë„ ë³€ê²½ ê°€ëŠ¥
        connection_string=os.getenv("PGVECTOR_CONNECTION_STRING")
    )

    # 4ï¸âƒ£ Elasticsearchì— ì œëª© + ìš”ì•½ + íƒœê·¸ ì €ì¥
    summary = ""
    tag_list = []
    try:
        summary_and_tags = summarize_and_tag(raw_text)
        summary = summary_and_tags["summary"]
        title = summary_and_tags["title"]
        tag_list = summary_and_tags["tags"]

        index_tip_document(text=raw_text, summary=summary, tags=tag_list)
    except Exception as e:
        print(f"[Elasticsearch ì €ì¥ ì¤‘ ì˜¤ë¥˜] {e}")

    return {
        "chunks": len(docs),
        "title": title,
        "summary": summary,
        "tags": tag_list
    }
